{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures.ResConv64 import *\r\n",
    "import torch\r\n",
    "import numpy as np\r\n",
    "from carla_disentanglement.datasets.dsprites import DSpritesDataset\r\n",
    "from models.annealed_vae import AnnealedVAE\r\n",
    "\r\n",
    "import torch.nn as nn\r\n",
    "import torch.distributions as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE_custom_conv(nn.Module):\r\n",
    "    def __init__(self, enc, dec, latent=10, channels=1, MNIST=False):\r\n",
    "        super(BetaVAE_custom_conv, self).__init__()\r\n",
    "\r\n",
    "        self.latent = latent\r\n",
    "        self.channels = channels\r\n",
    "        self.img_dim = 28 if MNIST else 64\r\n",
    "\r\n",
    "        self.encoder = enc\r\n",
    "\r\n",
    "        # Decoder\r\n",
    "        self.decoder = dec\r\n",
    "\r\n",
    "    def BottomUp(self, x):\r\n",
    "        mu, lv = self.encoder(x)\r\n",
    "        return mu.contiguous(), lv.contiguous()\r\n",
    "\r\n",
    "    def reparameterize(self, mu, lv):\r\n",
    "        std = lv.mul(0.5).exp()\r\n",
    "        z = td.Normal(mu, std).rsample()\r\n",
    "        return z.contiguous()\r\n",
    "\r\n",
    "    def TopDown(self, z):\r\n",
    "        # z = self.conv_prep(x)\r\n",
    "        # unflatten_dim = int(np.sqrt(self.conv_out_dim / self.filters[-1]))\r\n",
    "        # z = z.view(x.shape[0], self.filters[-1], unflatten_dim, unflatten_dim)\r\n",
    "        out = self.decoder(z)\r\n",
    "        return out\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        mu, lv = self.BottomUp(x)\r\n",
    "        z = self.reparameterize(mu, lv)\r\n",
    "        out = self.TopDown(z)\r\n",
    "        return torch.sigmoid(out)\r\n",
    "\r\n",
    "    def calc_loss(self, x, beta):\r\n",
    "        mu, lv = self.BottomUp(x)\r\n",
    "        z = self.reparameterize(mu, lv)\r\n",
    "        out = torch.sigmoid(self.TopDown(z))\r\n",
    "\r\n",
    "        # zeros = torch.zeros_like(mu).detach()\r\n",
    "        # ones = torch.ones_like(lv).detach()\r\n",
    "        # p_x = td.Normal(loc=zeros, scale=ones)\r\n",
    "        # q_zGx = td.Normal(loc=mu, scale=lv.mul(0.5).exp())\r\n",
    "        # kl = td.kl_divergence(q_zGx, p_x).sum()# / x.shape[0]\r\n",
    "\r\n",
    "        # x = x*0.3081 + 0.1307\r\n",
    "        # nll = td.Bernoulli(logits=out).log_prob(x).sum() / x.shape[0]\r\n",
    "        # BCEWithLogitsLoss because binary_cross_entropy_with_logits will not accepts reduction = none\r\n",
    "        # nll = -nn.BCEWithLogitsLoss(reduction='none')(out, x).sum()# / x.shape[0]\r\n",
    "\r\n",
    "        nll = -nn.functional.binary_cross_entropy(out, x, reduction='sum') / x.shape[0]\r\n",
    "        kl = (-0.5 * torch.sum(1 + lv - mu.pow(2) - lv.exp()) + 1e-5) / x.shape[0]\r\n",
    "        # print(kl, nll, out.min(), out.max())\r\n",
    "\r\n",
    "        return (-nll + kl * beta).contiguous(), kl, nll, out\r\n",
    "\r\n",
    "    def LT_fitted_gauss_2std(self, x, num_var=6, num_traversal=5, gif_fps=5, silent=False):\r\n",
    "        # Cycle linearly through +-2 std dev of a fitted Gaussian.\r\n",
    "        mu, lv = self.BottomUp(x)\r\n",
    "        num_traversal += 1 if num_traversal % 2 == 0 else num_traversal\r\n",
    "\r\n",
    "        for i, batch_mu in enumerate(mu[:num_var]):\r\n",
    "            images = []\r\n",
    "            images.append(torch.sigmoid(self.TopDown(batch_mu.unsqueeze(0))))\r\n",
    "            for latent_var in range(batch_mu.shape[0]):\r\n",
    "                new_mu = batch_mu.unsqueeze(0).repeat([num_traversal, 1])\r\n",
    "                loc = mu[:, latent_var].mean()\r\n",
    "                total_var = lv[:, latent_var].exp().mean() + mu[:, latent_var].var()\r\n",
    "                scale = total_var.sqrt()\r\n",
    "\r\n",
    "                # gif\r\n",
    "                new_mu[:, latent_var] = cycle_interval(batch_mu[latent_var], num_traversal,\r\n",
    "                                                       loc - 2 * scale, loc + 2 * scale)\r\n",
    "                filename = os.path.join(os.getcwd(), \"figures/mu_gifs/mu%d_var%d.gif\" % (i+1,latent_var+1))\r\n",
    "                save_animation(torch.sigmoid(self.TopDown(new_mu)), filename, num_traversal, fps=gif_fps)  #gif\r\n",
    "\r\n",
    "                # Plot\r\n",
    "                new_mu[:, latent_var] = torch.linspace((loc - 2 * scale).item(),\r\n",
    "                                                       (loc + 2 * scale).item(),\r\n",
    "                                                       steps = num_traversal)\r\n",
    "                images.append(torch.sigmoid(self.TopDown(new_mu)))\r\n",
    "\r\n",
    "            img_name = os.path.join(os.getcwd(), \"figures/traversals/Traversal%d.pdf\" % (i+1))\r\n",
    "            traversal_plotting(images, img_name, num_traversals=num_traversal, silent=silent)  # Traversal image\r\n",
    "        return images\r\n",
    "\r\n",
    "    def get_latent(self, x):\r\n",
    "        mu, _ = self.BottomUp(x)\r\n",
    "        return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DSpritesDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 10\r\n",
    "num_channels = 1\r\n",
    "image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "enc = GaussianResConv64(z_dim, num_channels, image_size)\r\n",
    "dec = ResConv64Decoder(z_dim, num_channels, image_size)\r\n",
    "net1 = AnnealedVAE(enc, dec, gamma=100.0, max_c=20, iterations_c=1e5, reconstruction='bce')\r\n",
    "net2 = BetaVAE_custom_conv(enc, dec, z_dim, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = ds[0][0].unsqueeze(0)\r\n",
    "net1.model.cpu()\r\n",
    "\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z1 = net1.model.encode(x)\r\n",
    "\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z2 = net1.model.encode(x)\r\n",
    "\r\n",
    "print(z1[0] - z2[0])\r\n",
    "print(z1[1] - z2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net2.cpu()\r\n",
    "\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z1 = net2.BottomUp(x)\r\n",
    "\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z2 = net2.BottomUp(x)\r\n",
    "\r\n",
    "print(z1[0] - z2[0])\r\n",
    "print(z1[1] - z2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z1 = net1.model.encode(x)\r\n",
    "\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z2 = net2.BottomUp(x)\r\n",
    "\r\n",
    "print(z1[0] - z2[0])\r\n",
    "print(z1[1] - z2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z1 = net1.model.encode(x)\r\n",
    "z1_ = net1.model.reparametrize(z1[0],z1[1])\r\n",
    "\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z2 = net2.BottomUp(x)\r\n",
    "z2_ = net2.reparameterize(z2[0],z2[1])\r\n",
    "\r\n",
    "print(z1_ - z2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<SubBackward0>)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z1 = net1.model.forward(x)\r\n",
    "\r\n",
    "\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z2 = net2.forward(x)\r\n",
    "\r\n",
    "print(torch.sigmoid(z1[0]) - z2[0])\r\n",
    "print((torch.sigmoid(z1[0]) - z2[0]).sum())\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0012], grad_fn=<SubBackward0>)\n",
      "tensor([-0.0010], grad_fn=<SubBackward0>)\n",
      "0.0\n",
      "tensor(-1.1444e-05, grad_fn=<SubBackward0>)\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "y1, (mu1, log_var1), _ = net1.model.forward(x)\r\n",
    "recon_loss1 = net1.reconstruction_loss(y1, x).div(1)\r\n",
    "# reg_loss1 = net1.beta * net1.gauss_vae_regulariser(mu1, log_var1)\r\n",
    "capacity = min(net1.max_c, net1.max_c * net1.train_iter_f / net1.iterations_c)\r\n",
    "kld1 = ((torch.exp(log_var1) + mu1*mu1 - log_var1 - 1)/2).sum(1).mean(0)\r\n",
    "kld_c1 = (kld1 - capacity).abs()\r\n",
    "reg_loss1 = net1.beta *kld_c1\r\n",
    "loss1 = recon_loss1 + reg_loss1\r\n",
    "\r\n",
    "\r\n",
    "C_max = torch.Tensor([20])\r\n",
    "C_stop_iter = 1e5\r\n",
    "global_iter = 0\r\n",
    "gamma = 100\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "loss2_, kl, nll, y2 = net2.calc_loss(x, beta=99)\r\n",
    "C = torch.clamp(C_max/C_stop_iter*global_iter, 0, C_max.item())\r\n",
    "loss2 = -nll + gamma*(kl-C).abs()\r\n",
    "\r\n",
    "print((torch.sigmoid(y1) - y2).sum())\r\n",
    "print(recon_loss1 + nll)\r\n",
    "print(reg_loss1 - gamma*(kl-C).abs())\r\n",
    "print(loss1 - loss2)\r\n",
    "\r\n",
    "print(net1.beta - gamma)\r\n",
    "print(kld1 - kl)\r\n",
    "print(capacity - C)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor(-1.1444e-05, grad_fn=<SubBackward0>)\n",
      "tensor(0., grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z1 = net1.model.encode(x)\r\n",
    "\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "z2 = net2.BottomUp(x)\r\n",
    "\r\n",
    "print(z1[0] - z2[0])\r\n",
    "print(z1[1] - z2[1])\r\n",
    "\r\n",
    "mu1, log_var1 = z1\r\n",
    "kl1 = ((torch.exp(log_var1) + mu1*mu1 - log_var1 - 1)/2).sum(1).mean(0)\r\n",
    "\r\n",
    "mu2, lv2 = z2\r\n",
    "kl2 = (-0.5 * torch.sum(1 + lv2 - mu2.pow(2) - lv2.exp())+ 1e-5) / x.shape[0] \r\n",
    "kl2_ = (-0.5 * torch.sum(1 + lv2 - mu2.pow(2) - lv2.exp())) / x.shape[0] \r\n",
    "\r\n",
    "print(kl1 -kl2)\r\n",
    "print(kl1 -kl2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(0.0005, grad_fn=<AddBackward0>)\n",
      "tensor([-0.0012], grad_fn=<SubBackward0>)\n",
      "tensor([-0.0010], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "y1, (mu1, log_var1), _ = net1.model.forward(x)\r\n",
    "recon_loss1 = net1.reconstruction_loss(y1, x).div(1)\r\n",
    "net1.global_iter = 100\r\n",
    "reg_loss1 = net1.beta * net1.gauss_vae_regulariser(mu1, log_var1)\r\n",
    "loss1 = recon_loss1 + reg_loss1\r\n",
    "\r\n",
    "\r\n",
    "C_max = torch.Tensor([20])\r\n",
    "C_stop_iter = 1e5\r\n",
    "global_iter = 100\r\n",
    "gamma = 100\r\n",
    "torch.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "loss2_, kl, nll, y2 = net2.calc_loss(x, beta=99)\r\n",
    "C = torch.clamp(C_max/C_stop_iter*global_iter, 0, C_max.item())\r\n",
    "loss2 = -nll + gamma*(kl-C).abs()\r\n",
    "\r\n",
    "print((torch.sigmoid(y1) - y2).sum())\r\n",
    "print(recon_loss1 + nll)\r\n",
    "print(reg_loss1 - gamma*(kl-C).abs())\r\n",
    "print(loss1 - loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "376c552cebb382e2be390b797e042089a374e3fa7ee98255c61a2c1ffbc8acc5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('carla_disentanglement': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}