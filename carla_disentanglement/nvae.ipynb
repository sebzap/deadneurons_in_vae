{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('carla_disentanglement': conda)"
  },
  "interpreter": {
   "hash": "376c552cebb382e2be390b797e042089a374e3fa7ee98255c61a2c1ffbc8acc5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import time\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from torchvision.datasets import ImageFolder\r\n",
    "from torchvision import transforms\r\n",
    "\r\n",
    "transform = transforms.Compose([\r\n",
    "    transforms.ToTensor(),\r\n",
    "])\r\n",
    "ds = ImageFolder(\"./data/splits/\", transform=transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def groups_per_scale(num_scales, num_groups_per_scale, is_adaptive, divider=2, minimum_groups=1):\r\n",
    "    g = []\r\n",
    "    n = num_groups_per_scale\r\n",
    "    for s in range(num_scales):\r\n",
    "        assert n >= 1\r\n",
    "        g.append(n)\r\n",
    "        if is_adaptive:\r\n",
    "            n = n // divider\r\n",
    "            n = max(minimum_groups, n)\r\n",
    "    return g\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "CHANNEL_MULT = 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from nvae.neural_operations import OPS, EncCombinerCell, DecCombinerCell, Conv2D, get_skip_connection, SE\r\n",
    "from nvae.neural_ar_operations import ARConv2d, ARInvertedResidual, MixLogCDFParam, mix_log_cdf_flow\r\n",
    "from nvae.neural_ar_operations import ELUConv as ARELUConv\r\n",
    "from nvae.utils import get_arch_cells, get_stride_for_cell_type, get_input_size, groups_per_scale\r\n",
    "from thirdparty.inplaced_sync_batchnorm import SyncBatchNormSwish\r\n",
    "from nvae.distributions import Normal, DiscMixLogistic"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Cell(nn.Module):\r\n",
    "    def __init__(self, Cin, Cout, cell_type, arch, use_se):\r\n",
    "        super(Cell, self).__init__()\r\n",
    "        self.cell_type = cell_type\r\n",
    "\r\n",
    "        stride = get_stride_for_cell_type(self.cell_type)\r\n",
    "        self.skip = get_skip_connection(Cin, stride, affine=False, channel_mult=CHANNEL_MULT)\r\n",
    "        self.use_se = use_se\r\n",
    "        self._num_nodes = len(arch)\r\n",
    "        self._ops = nn.ModuleList()\r\n",
    "        for i in range(self._num_nodes):\r\n",
    "            stride = get_stride_for_cell_type(self.cell_type) if i == 0 else 1\r\n",
    "            C = Cin if i == 0 else Cout\r\n",
    "            primitive = arch[i]\r\n",
    "            op = OPS[primitive](C, Cout, stride)\r\n",
    "            self._ops.append(op)\r\n",
    "\r\n",
    "        # SE\r\n",
    "        if self.use_se:\r\n",
    "            self.se = SE(Cout, Cout)\r\n",
    "\r\n",
    "    def forward(self, s):\r\n",
    "        # skip branch\r\n",
    "        skip = self.skip(s)\r\n",
    "        for i in range(self._num_nodes):\r\n",
    "            s = self._ops[i](s)\r\n",
    "\r\n",
    "        s = self.se(s) if self.use_se else s\r\n",
    "        return skip + 0.1 * s\r\n",
    "\r\n",
    "\r\n",
    "class CellAR(nn.Module):\r\n",
    "    def __init__(self, num_z, num_ftr, num_c, arch, mirror):\r\n",
    "        super(CellAR, self).__init__()\r\n",
    "        assert num_c % num_z == 0\r\n",
    "\r\n",
    "        self.cell_type = 'ar_nn'\r\n",
    "\r\n",
    "        # s0 will the random samples\r\n",
    "        ex = 6\r\n",
    "        self.conv = ARInvertedResidual(num_z, num_ftr, ex=ex, mirror=mirror)\r\n",
    "\r\n",
    "        self.use_mix_log_cdf = False\r\n",
    "        if self.use_mix_log_cdf:\r\n",
    "            self.param = MixLogCDFParam(num_z, num_mix=3, num_ftr=self.conv.hidden_dim, mirror=mirror)\r\n",
    "        else:\r\n",
    "            # 0.1 helps bring mu closer to 0 initially\r\n",
    "            self.mu = ARELUConv(self.conv.hidden_dim, num_z, kernel_size=1, padding=0, masked=True, zero_diag=False,\r\n",
    "                                weight_init_coeff=0.1, mirror=mirror)\r\n",
    "\r\n",
    "    def forward(self, z, ftr):\r\n",
    "        s = self.conv(z, ftr)\r\n",
    "\r\n",
    "        if self.use_mix_log_cdf:\r\n",
    "            logit_pi, mu, log_s, log_a, b = self.param(s)\r\n",
    "            new_z, log_det = mix_log_cdf_flow(z, logit_pi, mu, log_s, log_a, b)\r\n",
    "        else:\r\n",
    "            mu = self.mu(s)\r\n",
    "            new_z = (z - mu)\r\n",
    "            log_det = torch.zeros_like(new_z)\r\n",
    "\r\n",
    "        return new_z, log_det\r\n",
    "\r\n",
    "class PairedCellAR(nn.Module):\r\n",
    "    def __init__(self, num_z, num_ftr, num_c, arch=None):\r\n",
    "        super(PairedCellAR, self).__init__()\r\n",
    "        self.cell1 = CellAR(num_z, num_ftr, num_c, arch, mirror=False)\r\n",
    "        self.cell2 = CellAR(num_z, num_ftr, num_c, arch, mirror=True)\r\n",
    "\r\n",
    "    def forward(self, z, ftr):\r\n",
    "        new_z, log_det1 = self.cell1(z, ftr)\r\n",
    "        new_z, log_det2 = self.cell2(new_z, ftr)\r\n",
    "\r\n",
    "        log_det1 += log_det2\r\n",
    "        return new_z, log_det1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class AutoEncoder(nn.Module):\r\n",
    "    def __init__(self, ds):\r\n",
    "        super(AutoEncoder, self).__init__()\r\n",
    "        self.writer = None\r\n",
    "        self.arch_instance = get_arch_cells('res_mbconv')\r\n",
    "        self.dataset = ds\r\n",
    "        self.crop_output = self.dataset == 'mnist'\r\n",
    "        self.use_se = True\r\n",
    "        self.res_dist = True\r\n",
    "        self.num_bits = 5\r\n",
    "\r\n",
    "        self.num_latent_scales = 2 #args.num_latent_scales         # number of spatial scales that latent layers will reside\r\n",
    "        self.num_groups_per_scale = 10 #args.num_groups_per_scale   # number of groups of latent vars. per scale\r\n",
    "        self.num_latent_per_group = 10 #args.num_latent_per_group   # number of latent vars. per group\r\n",
    "        self.groups_per_scale = groups_per_scale(self.num_latent_scales, self.num_groups_per_scale, True,\r\n",
    "                                                 minimum_groups=4)\r\n",
    "\r\n",
    "        self.vanilla_vae = self.num_latent_scales == 1 and self.num_groups_per_scale == 1\r\n",
    "\r\n",
    "        # encoder parameteres\r\n",
    "        self.num_channels_enc = 16#args.num_channels_enc\r\n",
    "        self.num_channels_dec = 16#args.num_channels_dec\r\n",
    "        self.num_preprocess_blocks = 2#args.num_preprocess_blocks  # block is defined as series of Normal followed by Down\r\n",
    "        self.num_preprocess_cells = 3#args.num_preprocess_cells   # number of cells per block\r\n",
    "        self.num_cell_per_cond_enc = 2#args.num_cell_per_cond_enc  # number of cell for each conditional in encoder\r\n",
    "\r\n",
    "        # decoder parameters\r\n",
    "        # self.num_channels_dec = args.num_channels_dec\r\n",
    "        self.num_postprocess_blocks = 2#args.num_postprocess_blocks\r\n",
    "        self.num_postprocess_cells = 3#args.num_postprocess_cells\r\n",
    "        self.num_cell_per_cond_dec = 2#args.num_cell_per_cond_dec  # number of cell for each conditional in decoder\r\n",
    "\r\n",
    "        # general cell parameters\r\n",
    "        self.input_size = 56#get_input_size(self.dataset)\r\n",
    "\r\n",
    "        # decoder param\r\n",
    "        self.num_mix_output = 10\r\n",
    "\r\n",
    "        # used for generative purpose\r\n",
    "        c_scaling = CHANNEL_MULT ** (self.num_preprocess_blocks + self.num_latent_scales - 1)\r\n",
    "        spatial_scaling = 2 ** (self.num_preprocess_blocks + self.num_latent_scales - 1)\r\n",
    "        prior_ftr0_size = (int(c_scaling * self.num_channels_dec), self.input_size // spatial_scaling,\r\n",
    "                           self.input_size // spatial_scaling)\r\n",
    "        self.prior_ftr0 = nn.Parameter(torch.rand(size=prior_ftr0_size), requires_grad=True)\r\n",
    "        self.z0_size = [self.num_latent_per_group, self.input_size // spatial_scaling, self.input_size // spatial_scaling]\r\n",
    "\r\n",
    "        self.stem = self.init_stem()\r\n",
    "        self.pre_process, mult = self.init_pre_process(mult=1)\r\n",
    "\r\n",
    "        if self.vanilla_vae:\r\n",
    "            self.enc_tower = []\r\n",
    "        else:\r\n",
    "            self.enc_tower, mult = self.init_encoder_tower(mult)\r\n",
    "\r\n",
    "        self.with_nf = False #args.num_nf > 0\r\n",
    "        self.num_flows = 0#args.num_nf\r\n",
    "\r\n",
    "        self.enc0 = self.init_encoder0(mult)\r\n",
    "        self.enc_sampler, self.dec_sampler, self.nf_cells, self.enc_kv, self.dec_kv, self.query = \\\r\n",
    "            self.init_normal_sampler(mult)\r\n",
    "\r\n",
    "        if self.vanilla_vae:\r\n",
    "            self.dec_tower = []\r\n",
    "            self.stem_decoder = Conv2D(self.num_latent_per_group, mult * self.num_channels_enc, (1, 1), bias=True)\r\n",
    "        else:\r\n",
    "            self.dec_tower, mult = self.init_decoder_tower(mult)\r\n",
    "\r\n",
    "        self.post_process, mult = self.init_post_process(mult)\r\n",
    "\r\n",
    "        self.image_conditional = self.init_image_conditional(mult)\r\n",
    "\r\n",
    "        # collect all norm params in Conv2D and gamma param in batchnorm\r\n",
    "        self.all_log_norm = []\r\n",
    "        self.all_conv_layers = []\r\n",
    "        self.all_bn_layers = []\r\n",
    "        for n, layer in self.named_modules():\r\n",
    "            # if isinstance(layer, Conv2D) and '_ops' in n:   # only chose those in cell\r\n",
    "            if isinstance(layer, Conv2D) or isinstance(layer, ARConv2d):\r\n",
    "                self.all_log_norm.append(layer.log_weight_norm)\r\n",
    "                self.all_conv_layers.append(layer)\r\n",
    "            if isinstance(layer, nn.BatchNorm2d) or isinstance(layer, nn.SyncBatchNorm) or \\\r\n",
    "                    isinstance(layer, SyncBatchNormSwish):\r\n",
    "                self.all_bn_layers.append(layer)\r\n",
    "\r\n",
    "        print('len log norm:', len(self.all_log_norm))\r\n",
    "        print('len bn:', len(self.all_bn_layers))\r\n",
    "        # left/right singular vectors used for SR\r\n",
    "        self.sr_u = {}\r\n",
    "        self.sr_v = {}\r\n",
    "        self.num_power_iter = 4\r\n",
    "\r\n",
    "    def init_stem(self):\r\n",
    "        Cout = self.num_channels_enc\r\n",
    "        Cin = 1 if self.dataset == 'mnist' else 3\r\n",
    "        stem = Conv2D(Cin, Cout, 3, padding=1, bias=True)\r\n",
    "        return stem\r\n",
    "\r\n",
    "    def init_pre_process(self, mult):\r\n",
    "        pre_process = nn.ModuleList()\r\n",
    "        for b in range(self.num_preprocess_blocks):\r\n",
    "            for c in range(self.num_preprocess_cells):\r\n",
    "                if c == self.num_preprocess_cells - 1:\r\n",
    "                    arch = self.arch_instance['down_pre']\r\n",
    "                    num_ci = int(self.num_channels_enc * mult)\r\n",
    "                    num_co = int(CHANNEL_MULT * num_ci)\r\n",
    "                    cell = Cell(num_ci, num_co, cell_type='down_pre', arch=arch, use_se=self.use_se)\r\n",
    "                    mult = CHANNEL_MULT * mult\r\n",
    "                else:\r\n",
    "                    arch = self.arch_instance['normal_pre']\r\n",
    "                    num_c = self.num_channels_enc * mult\r\n",
    "                    cell = Cell(num_c, num_c, cell_type='normal_pre', arch=arch, use_se=self.use_se)\r\n",
    "\r\n",
    "                pre_process.append(cell)\r\n",
    "\r\n",
    "        return pre_process, mult\r\n",
    "\r\n",
    "    def init_encoder_tower(self, mult):\r\n",
    "        enc_tower = nn.ModuleList()\r\n",
    "        for s in range(self.num_latent_scales):\r\n",
    "            for g in range(self.groups_per_scale[s]):\r\n",
    "                for c in range(self.num_cell_per_cond_enc):\r\n",
    "                    arch = self.arch_instance['normal_enc']\r\n",
    "                    num_c = int(self.num_channels_enc * mult)\r\n",
    "                    cell = Cell(num_c, num_c, cell_type='normal_enc', arch=arch, use_se=self.use_se)\r\n",
    "                    enc_tower.append(cell)\r\n",
    "\r\n",
    "                # add encoder combiner\r\n",
    "                if not (s == self.num_latent_scales - 1 and g == self.groups_per_scale[s] - 1):\r\n",
    "                    num_ce = int(self.num_channels_enc * mult)\r\n",
    "                    num_cd = int(self.num_channels_dec * mult)\r\n",
    "                    cell = EncCombinerCell(num_ce, num_cd, num_ce, cell_type='combiner_enc')\r\n",
    "                    enc_tower.append(cell)\r\n",
    "\r\n",
    "            # down cells after finishing a scale\r\n",
    "            if s < self.num_latent_scales - 1:\r\n",
    "                arch = self.arch_instance['down_enc']\r\n",
    "                num_ci = int(self.num_channels_enc * mult)\r\n",
    "                num_co = int(CHANNEL_MULT * num_ci)\r\n",
    "                cell = Cell(num_ci, num_co, cell_type='down_enc', arch=arch, use_se=self.use_se)\r\n",
    "                enc_tower.append(cell)\r\n",
    "                mult = CHANNEL_MULT * mult\r\n",
    "\r\n",
    "        return enc_tower, mult\r\n",
    "\r\n",
    "    def init_encoder0(self, mult):\r\n",
    "        num_c = int(self.num_channels_enc * mult)\r\n",
    "        cell = nn.Sequential(\r\n",
    "            nn.ELU(),\r\n",
    "            Conv2D(num_c, num_c, kernel_size=1, bias=True),\r\n",
    "            nn.ELU())\r\n",
    "        return cell\r\n",
    "\r\n",
    "    def init_normal_sampler(self, mult):\r\n",
    "        enc_sampler, dec_sampler, nf_cells = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\r\n",
    "        enc_kv, dec_kv, query = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\r\n",
    "        for s in range(self.num_latent_scales):\r\n",
    "            for g in range(self.groups_per_scale[self.num_latent_scales - s - 1]):\r\n",
    "                # build mu, sigma generator for encoder\r\n",
    "                num_c = int(self.num_channels_enc * mult)\r\n",
    "                cell = Conv2D(num_c, 2 * self.num_latent_per_group, kernel_size=3, padding=1, bias=True)\r\n",
    "                enc_sampler.append(cell)\r\n",
    "                # build NF\r\n",
    "                for n in range(self.num_flows):\r\n",
    "                    arch = self.arch_instance['ar_nn']\r\n",
    "                    num_c1 = int(self.num_channels_enc * mult)\r\n",
    "                    num_c2 = 8 * self.num_latent_per_group  # use 8x features\r\n",
    "                    nf_cells.append(PairedCellAR(self.num_latent_per_group, num_c1, num_c2, arch))\r\n",
    "                if not (s == 0 and g == 0):  # for the first group, we use a fixed standard Normal.\r\n",
    "                    num_c = int(self.num_channels_dec * mult)\r\n",
    "                    cell = nn.Sequential(\r\n",
    "                        nn.ELU(),\r\n",
    "                        Conv2D(num_c, 2 * self.num_latent_per_group, kernel_size=1, padding=0, bias=True))\r\n",
    "                    dec_sampler.append(cell)\r\n",
    "\r\n",
    "            mult = mult / CHANNEL_MULT\r\n",
    "\r\n",
    "        return enc_sampler, dec_sampler, nf_cells, enc_kv, dec_kv, query\r\n",
    "\r\n",
    "    def init_decoder_tower(self, mult):\r\n",
    "        # create decoder tower\r\n",
    "        dec_tower = nn.ModuleList()\r\n",
    "        for s in range(self.num_latent_scales):\r\n",
    "            for g in range(self.groups_per_scale[self.num_latent_scales - s - 1]):\r\n",
    "                num_c = int(self.num_channels_dec * mult)\r\n",
    "                if not (s == 0 and g == 0):\r\n",
    "                    for c in range(self.num_cell_per_cond_dec):\r\n",
    "                        arch = self.arch_instance['normal_dec']\r\n",
    "                        cell = Cell(num_c, num_c, cell_type='normal_dec', arch=arch, use_se=self.use_se)\r\n",
    "                        dec_tower.append(cell)\r\n",
    "\r\n",
    "                cell = DecCombinerCell(num_c, self.num_latent_per_group, num_c, cell_type='combiner_dec')\r\n",
    "                dec_tower.append(cell)\r\n",
    "\r\n",
    "            # down cells after finishing a scale\r\n",
    "            if s < self.num_latent_scales - 1:\r\n",
    "                arch = self.arch_instance['up_dec']\r\n",
    "                num_ci = int(self.num_channels_dec * mult)\r\n",
    "                num_co = int(num_ci / CHANNEL_MULT)\r\n",
    "                cell = Cell(num_ci, num_co, cell_type='up_dec', arch=arch, use_se=self.use_se)\r\n",
    "                dec_tower.append(cell)\r\n",
    "                mult = mult / CHANNEL_MULT\r\n",
    "\r\n",
    "        return dec_tower, mult\r\n",
    "\r\n",
    "    def init_post_process(self, mult):\r\n",
    "        post_process = nn.ModuleList()\r\n",
    "        for b in range(self.num_postprocess_blocks):\r\n",
    "            for c in range(self.num_postprocess_cells):\r\n",
    "                if c == 0:\r\n",
    "                    arch = self.arch_instance['up_post']\r\n",
    "                    num_ci = int(self.num_channels_dec * mult)\r\n",
    "                    num_co = int(num_ci / CHANNEL_MULT)\r\n",
    "                    cell = Cell(num_ci, num_co, cell_type='up_post', arch=arch, use_se=self.use_se)\r\n",
    "                    mult = mult / CHANNEL_MULT\r\n",
    "                else:\r\n",
    "                    arch = self.arch_instance['normal_post']\r\n",
    "                    num_c = int(self.num_channels_dec * mult)\r\n",
    "                    cell = Cell(num_c, num_c, cell_type='normal_post', arch=arch, use_se=self.use_se)\r\n",
    "\r\n",
    "                post_process.append(cell)\r\n",
    "\r\n",
    "        return post_process, mult\r\n",
    "\r\n",
    "    def init_image_conditional(self, mult):\r\n",
    "        C_in = int(self.num_channels_dec * mult)\r\n",
    "        C_out = 1 if self.dataset == 'mnist' else 10 * self.num_mix_output\r\n",
    "        return nn.Sequential(nn.ELU(),\r\n",
    "                             Conv2D(C_in, C_out, 3, padding=1, bias=True))\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        s = self.stem(2 * x - 1.0)\r\n",
    "\r\n",
    "        # perform pre-processing\r\n",
    "        for cell in self.pre_process:\r\n",
    "            s = cell(s)\r\n",
    "\r\n",
    "        # run the main encoder tower\r\n",
    "        combiner_cells_enc = []\r\n",
    "        combiner_cells_s = []\r\n",
    "        for cell in self.enc_tower:\r\n",
    "            if cell.cell_type == 'combiner_enc':\r\n",
    "                combiner_cells_enc.append(cell)\r\n",
    "                combiner_cells_s.append(s)\r\n",
    "            else:\r\n",
    "                s = cell(s)\r\n",
    "\r\n",
    "        # reverse combiner cells and their input for decoder\r\n",
    "        combiner_cells_enc.reverse()\r\n",
    "        combiner_cells_s.reverse()\r\n",
    "\r\n",
    "        idx_dec = 0\r\n",
    "        ftr = self.enc0(s)                            # this reduces the channel dimension\r\n",
    "        param0 = self.enc_sampler[idx_dec](ftr)\r\n",
    "        mu_q, log_sig_q = torch.chunk(param0, 2, dim=1)\r\n",
    "        dist = Normal(mu_q, log_sig_q)   # for the first approx. posterior\r\n",
    "        z, _ = dist.sample()\r\n",
    "        log_q_conv = dist.log_p(z)\r\n",
    "\r\n",
    "        # apply normalizing flows\r\n",
    "        nf_offset = 0\r\n",
    "        for n in range(self.num_flows):\r\n",
    "            z, log_det = self.nf_cells[n](z, ftr)\r\n",
    "            log_q_conv -= log_det\r\n",
    "        nf_offset += self.num_flows\r\n",
    "        all_q = [dist]\r\n",
    "        all_log_q = [log_q_conv]\r\n",
    "\r\n",
    "        # To make sure we do not pass any deterministic features from x to decoder.\r\n",
    "        s = 0\r\n",
    "\r\n",
    "        # prior for z0\r\n",
    "        dist = Normal(mu=torch.zeros_like(z), log_sigma=torch.zeros_like(z))\r\n",
    "        log_p_conv = dist.log_p(z)\r\n",
    "        all_p = [dist]\r\n",
    "        all_log_p = [log_p_conv]\r\n",
    "\r\n",
    "        idx_dec = 0\r\n",
    "        s = self.prior_ftr0.unsqueeze(0)\r\n",
    "        batch_size = z.size(0)\r\n",
    "        s = s.expand(batch_size, -1, -1, -1)\r\n",
    "        for cell in self.dec_tower:\r\n",
    "            if cell.cell_type == 'combiner_dec':\r\n",
    "                if idx_dec > 0:\r\n",
    "                    # form prior\r\n",
    "                    param = self.dec_sampler[idx_dec - 1](s)\r\n",
    "                    mu_p, log_sig_p = torch.chunk(param, 2, dim=1)\r\n",
    "\r\n",
    "                    # form encoder\r\n",
    "                    ftr = combiner_cells_enc[idx_dec - 1](combiner_cells_s[idx_dec - 1], s)\r\n",
    "                    param = self.enc_sampler[idx_dec](ftr)\r\n",
    "                    mu_q, log_sig_q = torch.chunk(param, 2, dim=1)\r\n",
    "                    dist = Normal(mu_p + mu_q, log_sig_p + log_sig_q) if self.res_dist else Normal(mu_q, log_sig_q)\r\n",
    "                    z, _ = dist.sample()\r\n",
    "                    log_q_conv = dist.log_p(z)\r\n",
    "                    # apply NF\r\n",
    "                    for n in range(self.num_flows):\r\n",
    "                        z, log_det = self.nf_cells[nf_offset + n](z, ftr)\r\n",
    "                        log_q_conv -= log_det\r\n",
    "                    nf_offset += self.num_flows\r\n",
    "                    all_log_q.append(log_q_conv)\r\n",
    "                    all_q.append(dist)\r\n",
    "\r\n",
    "                    # evaluate log_p(z)\r\n",
    "                    dist = Normal(mu_p, log_sig_p)\r\n",
    "                    log_p_conv = dist.log_p(z)\r\n",
    "                    all_p.append(dist)\r\n",
    "                    all_log_p.append(log_p_conv)\r\n",
    "\r\n",
    "                # 'combiner_dec'\r\n",
    "                s = cell(s, z)\r\n",
    "                idx_dec += 1\r\n",
    "            else:\r\n",
    "                s = cell(s)\r\n",
    "\r\n",
    "        if self.vanilla_vae:\r\n",
    "            s = self.stem_decoder(z)\r\n",
    "\r\n",
    "        for cell in self.post_process:\r\n",
    "            s = cell(s)\r\n",
    "\r\n",
    "        logits = self.image_conditional(s)\r\n",
    "\r\n",
    "        # compute kl\r\n",
    "        kl_all = []\r\n",
    "        kl_diag = []\r\n",
    "        log_p, log_q = 0., 0.\r\n",
    "        for q, p, log_q_conv, log_p_conv in zip(all_q, all_p, all_log_q, all_log_p):\r\n",
    "            if self.with_nf:\r\n",
    "                kl_per_var = log_q_conv - log_p_conv\r\n",
    "            else:\r\n",
    "                kl_per_var = q.kl(p)\r\n",
    "\r\n",
    "            kl_diag.append(torch.mean(torch.sum(kl_per_var, dim=[2, 3]), dim=0))\r\n",
    "            kl_all.append(torch.sum(kl_per_var, dim=[1, 2, 3]))\r\n",
    "            log_q += torch.sum(log_q_conv, dim=[1, 2, 3])\r\n",
    "            log_p += torch.sum(log_p_conv, dim=[1, 2, 3])\r\n",
    "\r\n",
    "        return logits, log_q, log_p, kl_all, kl_diag\r\n",
    "\r\n",
    "    def sample(self, num_samples, t):\r\n",
    "        scale_ind = 0\r\n",
    "        z0_size = [num_samples] + self.z0_size\r\n",
    "        dist = Normal(mu=torch.zeros(z0_size).cuda(), log_sigma=torch.zeros(z0_size).cuda(), temp=t)\r\n",
    "        z, _ = dist.sample()\r\n",
    "\r\n",
    "        idx_dec = 0\r\n",
    "        s = self.prior_ftr0.unsqueeze(0)\r\n",
    "        batch_size = z.size(0)\r\n",
    "        s = s.expand(batch_size, -1, -1, -1)\r\n",
    "        for cell in self.dec_tower:\r\n",
    "            if cell.cell_type == 'combiner_dec':\r\n",
    "                if idx_dec > 0:\r\n",
    "                    # form prior\r\n",
    "                    param = self.dec_sampler[idx_dec - 1](s)\r\n",
    "                    mu, log_sigma = torch.chunk(param, 2, dim=1)\r\n",
    "                    dist = Normal(mu, log_sigma, t)\r\n",
    "                    z, _ = dist.sample()\r\n",
    "\r\n",
    "                # 'combiner_dec'\r\n",
    "                s = cell(s, z)\r\n",
    "                idx_dec += 1\r\n",
    "            else:\r\n",
    "                s = cell(s)\r\n",
    "                if cell.cell_type == 'up_dec':\r\n",
    "                    scale_ind += 1\r\n",
    "\r\n",
    "        if self.vanilla_vae:\r\n",
    "            s = self.stem_decoder(z)\r\n",
    "\r\n",
    "        for cell in self.post_process:\r\n",
    "            s = cell(s)\r\n",
    "\r\n",
    "        logits = self.image_conditional(s)\r\n",
    "        return logits\r\n",
    "\r\n",
    "    def decoder_output(self, logits):\r\n",
    "        if self.dataset == 'mnist':\r\n",
    "            return Bernoulli(logits=logits)\r\n",
    "        elif self.dataset in {'cifar10', 'celeba_64', 'celeba_256', 'imagenet_32', 'imagenet_64', 'ffhq',\r\n",
    "                              'lsun_bedroom_128', 'lsun_bedroom_256'}:\r\n",
    "            return DiscMixLogistic(logits, self.num_mix_output, num_bits=self.num_bits)\r\n",
    "        else:\r\n",
    "            return DiscMixLogistic(logits, self.num_mix_output, num_bits=self.num_bits)\r\n",
    "            raise NotImplementedError\r\n",
    "\r\n",
    "    def spectral_norm_parallel(self):\r\n",
    "        \"\"\" This method computes spectral normalization for all conv layers in parallel. This method should be called\r\n",
    "         after calling the forward method of all the conv layers in each iteration. \"\"\"\r\n",
    "\r\n",
    "        weights = {}   # a dictionary indexed by the shape of weights\r\n",
    "        for l in self.all_conv_layers:\r\n",
    "            weight = l.weight_normalized\r\n",
    "            weight_mat = weight.view(weight.size(0), -1)\r\n",
    "            if weight_mat.shape not in weights:\r\n",
    "                weights[weight_mat.shape] = []\r\n",
    "\r\n",
    "            weights[weight_mat.shape].append(weight_mat)\r\n",
    "\r\n",
    "        loss = 0\r\n",
    "        for i in weights:\r\n",
    "            weights[i] = torch.stack(weights[i], dim=0)\r\n",
    "            with torch.no_grad():\r\n",
    "                num_iter = self.num_power_iter\r\n",
    "                if i not in self.sr_u:\r\n",
    "                    num_w, row, col = weights[i].shape\r\n",
    "                    self.sr_u[i] = F.normalize(torch.ones(num_w, row).normal_(0, 1).cuda(), dim=1, eps=1e-3)\r\n",
    "                    self.sr_v[i] = F.normalize(torch.ones(num_w, col).normal_(0, 1).cuda(), dim=1, eps=1e-3)\r\n",
    "                    # increase the number of iterations for the first time\r\n",
    "                    num_iter = 10 * self.num_power_iter\r\n",
    "\r\n",
    "                for j in range(num_iter):\r\n",
    "                    # Spectral norm of weight equals to `u^T W v`, where `u` and `v`\r\n",
    "                    # are the first left and right singular vectors.\r\n",
    "                    # This power iteration produces approximations of `u` and `v`.\r\n",
    "                    self.sr_v[i] = F.normalize(torch.matmul(self.sr_u[i].unsqueeze(1), weights[i]).squeeze(1),\r\n",
    "                                               dim=1, eps=1e-3)  # bx1xr * bxrxc --> bx1xc --> bxc\r\n",
    "                    self.sr_u[i] = F.normalize(torch.matmul(weights[i], self.sr_v[i].unsqueeze(2)).squeeze(2),\r\n",
    "                                               dim=1, eps=1e-3)  # bxrxc * bxcx1 --> bxrx1  --> bxr\r\n",
    "\r\n",
    "            sigma = torch.matmul(self.sr_u[i].unsqueeze(1), torch.matmul(weights[i], self.sr_v[i].unsqueeze(2)))\r\n",
    "            loss += torch.sum(sigma)\r\n",
    "        return loss\r\n",
    "\r\n",
    "    def batchnorm_loss(self):\r\n",
    "        loss = 0\r\n",
    "        for l in self.all_bn_layers:\r\n",
    "            if l.affine:\r\n",
    "                loss += torch.max(torch.abs(l.weight))\r\n",
    "\r\n",
    "        return loss\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "seed = 0\r\n",
    "\r\n",
    "torch.manual_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "torch.cuda.manual_seed(seed)\r\n",
    "torch.cuda.manual_seed_all(seed)\r\n",
    "\r\n",
    "model = AutoEncoder(ds)\r\n",
    "model = model.cuda()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len log norm: 255\n",
      "len bn: 214\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "epochs = 400\r\n",
    "warmup_epochs =5\r\n",
    "learning_rate =1e-2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from thirdparty.adamax import Adamax\r\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "cnn_optimizer = Adamax(model.parameters(), learning_rate, weight_decay=3e-4, eps=1e-3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "cnn_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(cnn_optimizer, float(epochs - warmup_epochs - 1), eta_min=1e-4)\r\n",
    "grad_scalar = GradScaler(2**10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "train_size = int(0.75 * len(ds))\r\n",
    "test_size = len(ds) - train_size\r\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(ds, [train_size, test_size], generator=torch.Generator().manual_seed(0))\r\n",
    "\r\n",
    "data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=0)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import nvae.utils as utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "with torch.cuda.amp.autocast():\r\n",
    "    model.train()\r\n",
    "    global_step = 0\r\n",
    "    warmup_iters = len(data_loader) * warmup_epochs\r\n",
    "    for epoch in range(epochs):\r\n",
    "        if epoch > warmup_epochs:\r\n",
    "                cnn_scheduler.step()\r\n",
    "\r\n",
    "        alpha_i = utils.kl_balancer_coeff(num_scales=model.num_latent_scales,\r\n",
    "                                        groups_per_scale=model.groups_per_scale, fun='square')\r\n",
    "        nelbo = utils.AvgrageMeter()\r\n",
    "        model.train()\r\n",
    "        for step, x in enumerate(data_loader):\r\n",
    "            x = x[0] if len(x) > 1 else x\r\n",
    "            x = x.cuda()\r\n",
    "\r\n",
    "            # change bit length\r\n",
    "            x = utils.pre_process(x, 8)\r\n",
    "\r\n",
    "            # warm-up lr\r\n",
    "            if global_step < warmup_iters:\r\n",
    "                lr = learning_rate * float(global_step) / warmup_iters\r\n",
    "                for param_group in cnn_optimizer.param_groups:\r\n",
    "                    param_group['lr'] = lr\r\n",
    "\r\n",
    "            cnn_optimizer.zero_grad()\r\n",
    "            # with autocast():\r\n",
    "            logits, log_q, log_p, kl_all, kl_diag = model(x)\r\n",
    "\r\n",
    "            kl_anneal_portion=0.3\r\n",
    "            num_total_iter=len(data_loader) * epochs\r\n",
    "            kl_const_portion=0.0001\r\n",
    "            kl_const_coeff=0.0001\r\n",
    "\r\n",
    "            output = model.decoder_output(logits)\r\n",
    "            kl_coeff = utils.kl_coeff(global_step, kl_anneal_portion * num_total_iter,\r\n",
    "                                        kl_const_portion * num_total_iter, kl_const_coeff)\r\n",
    "\r\n",
    "            recon_loss = utils.reconstruction_loss(output, x, crop=model.crop_output)\r\n",
    "            balanced_kl, kl_coeffs, kl_vals = utils.kl_balancer(kl_all, kl_coeff, kl_balance=True, alpha_i=alpha_i)\r\n",
    "\r\n",
    "            nelbo_batch = recon_loss + balanced_kl\r\n",
    "            loss = torch.mean(nelbo_batch)\r\n",
    "            norm_loss = model.spectral_norm_parallel()\r\n",
    "            bn_loss = model.batchnorm_loss()\r\n",
    "\r\n",
    "            # get spectral regularization coefficient (lambda)\r\n",
    "            # if args.weight_decay_norm_anneal:\r\n",
    "            #     assert args.weight_decay_norm_init > 0 and args.weight_decay_norm > 0, 'init and final wdn should be positive.'\r\n",
    "            #     wdn_coeff = (1. - kl_coeff) * np.log(args.weight_decay_norm_init) + kl_coeff * np.log(args.weight_decay_norm)\r\n",
    "            #     wdn_coeff = np.exp(wdn_coeff)\r\n",
    "            # else:\r\n",
    "            wdn_coeff =  1e-2 #args.weight_decay_norm\r\n",
    "\r\n",
    "            loss += norm_loss * wdn_coeff + bn_loss * wdn_coeff\r\n",
    "\r\n",
    "            grad_scalar.scale(loss).backward()\r\n",
    "            # utils.average_gradients(model.parameters(), args.distributed)\r\n",
    "            grad_scalar.step(cnn_optimizer)\r\n",
    "            grad_scalar.update()\r\n",
    "            nelbo.update(loss.data, 1)\r\n",
    "\r\n",
    "            global_step += 1\r\n",
    "\r\n",
    "        print(\"\\n\\n\")\r\n",
    "        print(\"epcho\",epoch)\r\n",
    "        print(\"loss\",loss)\r\n",
    "        print(\"step\", global_step)\r\n",
    "        y = output.sample()\r\n",
    "        img_tensor = torch.cat([x,y], dim=-2)\r\n",
    "        img_tensor = torch.cat([img for img in img_tensor][:16], dim=-1)\r\n",
    "        to_image = transforms.Compose([\r\n",
    "                    # inverts normalization of image # implement unnormalize when we use normalization\r\n",
    "                    # transforms.Normalize(-means / stds, 1. / stds),\r\n",
    "                    transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\r\n",
    "                    transforms.ToPILImage()\r\n",
    "                ])\r\n",
    "        display(to_image(img_tensor))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[ 8.3008e-02,  8.4473e-02,  1.3452e-01,  ...,  8.6304e-02,\n",
      "            4.5258e-02,  6.5857e-02],\n",
      "          [ 9.0088e-02,  3.9368e-02,  1.0846e-01,  ...,  3.0716e-02,\n",
      "            1.9531e-03,  4.1809e-02],\n",
      "          [ 4.4556e-02,  9.9243e-02,  1.3037e-01,  ...,  4.8492e-02,\n",
      "            2.8320e-02,  6.7749e-02],\n",
      "          ...,\n",
      "          [ 5.9479e-02,  5.9814e-02,  9.3323e-02,  ...,  1.1609e-01,\n",
      "            1.1572e-01,  2.1118e-02],\n",
      "          [ 2.7222e-02,  6.4941e-02,  7.3120e-02,  ...,  9.3018e-02,\n",
      "            1.2598e-01,  4.7577e-02],\n",
      "          [ 7.1289e-02,  5.4047e-02,  3.8330e-02,  ...,  5.5054e-02,\n",
      "            6.8909e-02,  6.8970e-02]],\n",
      "\n",
      "         [[ 1.4526e-02,  7.8278e-03, -1.0773e-02,  ...,  5.7434e-02,\n",
      "            5.6183e-02,  9.9854e-02],\n",
      "          [ 8.7708e-02,  1.1926e-01,  7.5317e-02,  ...,  1.1682e-01,\n",
      "            6.2744e-02,  8.2153e-02],\n",
      "          [ 9.0393e-02,  5.3680e-02,  1.2268e-01,  ...,  7.6294e-02,\n",
      "            3.3691e-02,  5.7373e-02],\n",
      "          ...,\n",
      "          [ 1.3623e-01,  8.3435e-02,  6.4819e-02,  ...,  4.4861e-03,\n",
      "           -2.8076e-03, -3.9154e-02],\n",
      "          [ 8.3557e-02,  3.1799e-02,  2.1362e-02,  ...,  2.0020e-02,\n",
      "           -2.8595e-02, -3.3051e-02],\n",
      "          [ 5.8746e-03,  5.0537e-02,  9.1309e-02,  ...,  6.3629e-03,\n",
      "           -4.2633e-02, -3.5980e-02]],\n",
      "\n",
      "         [[-6.5369e-02, -1.2000e-01, -1.1212e-01,  ..., -3.7079e-02,\n",
      "           -1.3123e-02, -2.5879e-02],\n",
      "          [-1.0352e-01, -9.7778e-02, -8.1665e-02,  ...,  1.6541e-02,\n",
      "            4.2175e-02,  1.9470e-02],\n",
      "          [-2.1805e-02,  1.4038e-02, -3.0624e-02,  ..., -3.3813e-02,\n",
      "           -1.2909e-02, -1.2451e-02],\n",
      "          ...,\n",
      "          [-1.0010e-01, -7.7026e-02, -1.2054e-02,  ..., -6.1493e-02,\n",
      "           -9.7656e-02, -5.0232e-02],\n",
      "          [-1.4270e-01, -1.1920e-01,  8.3008e-03,  ..., -3.1708e-02,\n",
      "           -1.9806e-02, -8.4351e-02],\n",
      "          [-8.6670e-02, -4.2847e-02,  2.3499e-02,  ..., -1.8005e-03,\n",
      "           -1.0712e-02, -9.5825e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.2410e-02, -4.5471e-02, -6.2012e-02,  ..., -4.3335e-02,\n",
      "           -1.3580e-03, -4.9286e-03],\n",
      "          [ 3.4729e-02, -1.3525e-01, -2.0593e-01,  ...,  7.5226e-03,\n",
      "           -4.8248e-02, -6.2134e-02],\n",
      "          [ 1.0757e-02, -1.2213e-01, -1.6528e-01,  ...,  1.9485e-02,\n",
      "           -4.3274e-02, -7.0374e-02],\n",
      "          ...,\n",
      "          [-3.6499e-02, -6.1951e-02, -1.1066e-01,  ..., -3.6835e-02,\n",
      "           -2.2827e-02, -1.7981e-01],\n",
      "          [ 4.4861e-02,  3.7476e-02, -3.1799e-02,  ..., -7.7881e-02,\n",
      "           -5.3223e-02, -1.6809e-01],\n",
      "          [ 7.0007e-02,  6.3293e-02, -6.3843e-02,  ..., -1.3770e-01,\n",
      "           -1.2512e-01, -6.9336e-02]],\n",
      "\n",
      "         [[-1.7853e-03, -6.7993e-02, -7.7026e-02,  ...,  6.3477e-03,\n",
      "            7.5684e-03,  3.6926e-02],\n",
      "          [ 3.4790e-02, -6.4514e-02,  3.8208e-02,  ...,  5.1758e-02,\n",
      "            1.0309e-01,  8.0688e-02],\n",
      "          [-1.0689e-02, -3.0090e-02,  4.3640e-02,  ...,  1.4877e-02,\n",
      "            7.6782e-02,  4.4983e-02],\n",
      "          ...,\n",
      "          [-1.0004e-01, -1.4404e-02, -1.0437e-02,  ..., -5.9021e-02,\n",
      "           -4.2603e-02, -1.3931e-02],\n",
      "          [-1.5540e-01, -6.1951e-02, -4.0970e-03,  ...,  2.6230e-02,\n",
      "            1.4633e-02,  8.0261e-03],\n",
      "          [-9.4299e-02, -2.4872e-02,  3.6072e-02,  ...,  3.1143e-02,\n",
      "            2.5284e-02,  5.4260e-02]],\n",
      "\n",
      "         [[-3.9520e-02, -4.1962e-02, -9.5276e-02,  ...,  4.2755e-02,\n",
      "            4.4861e-02, -1.1276e-02],\n",
      "          [-4.7058e-02,  3.5309e-02, -1.4992e-03,  ...,  1.0254e-02,\n",
      "            4.6577e-03,  1.6418e-02],\n",
      "          [-6.6345e-02, -3.1319e-03, -4.6814e-02,  ...,  5.7312e-02,\n",
      "            2.6894e-03,  6.9160e-03],\n",
      "          ...,\n",
      "          [ 4.3121e-02,  5.6885e-02,  6.2988e-02,  ..., -7.9407e-02,\n",
      "           -6.3171e-02, -3.6621e-02],\n",
      "          [ 5.7190e-02,  1.0461e-01,  6.6895e-02,  ..., -3.5980e-02,\n",
      "           -6.5491e-02, -2.4261e-02],\n",
      "          [ 1.8463e-02,  9.1064e-02,  5.9570e-02,  ..., -5.4840e-02,\n",
      "           -3.2043e-02,  1.3084e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.4434e-02,  2.5330e-02,  3.4546e-02,  ...,  9.2651e-02,\n",
      "            8.0139e-02,  3.4241e-02],\n",
      "          [ 5.1483e-02, -4.3030e-03, -6.0883e-02,  ...,  1.2219e-01,\n",
      "            1.4661e-01,  9.6069e-02],\n",
      "          [-3.2959e-03, -4.1809e-03, -4.5624e-02,  ...,  9.2712e-02,\n",
      "            1.1438e-01,  8.8562e-02],\n",
      "          ...,\n",
      "          [ 8.2214e-02,  5.7373e-02,  5.4779e-02,  ...,  1.1340e-01,\n",
      "            9.5337e-02,  1.0876e-01],\n",
      "          [ 7.0190e-02,  6.8542e-02,  7.3975e-02,  ...,  6.7261e-02,\n",
      "            4.1992e-02,  7.0801e-02],\n",
      "          [ 8.3740e-02,  7.1045e-02,  7.7454e-02,  ...,  6.9092e-02,\n",
      "            7.0984e-02,  7.7087e-02]],\n",
      "\n",
      "         [[-2.9297e-03, -2.8046e-02, -1.0559e-02,  ...,  1.6138e-01,\n",
      "            1.5088e-01,  1.0303e-01],\n",
      "          [-3.6652e-02, -4.4342e-02, -1.1108e-02,  ...,  1.3660e-01,\n",
      "            8.9294e-02,  4.6600e-02],\n",
      "          [-3.4576e-02, -6.0760e-02, -3.2562e-02,  ...,  1.2646e-01,\n",
      "            5.5847e-02,  5.1086e-02],\n",
      "          ...,\n",
      "          [ 5.3619e-02,  7.1899e-02,  4.2297e-02,  ..., -4.3030e-03,\n",
      "            8.9478e-02,  7.6904e-02],\n",
      "          [ 6.0242e-02,  4.7699e-02,  2.4109e-02,  ..., -1.9867e-02,\n",
      "            5.1086e-02,  8.0994e-02],\n",
      "          [ 2.8412e-02,  1.7761e-02,  1.6052e-02,  ..., -1.9592e-02,\n",
      "            2.0432e-02,  3.0960e-02]],\n",
      "\n",
      "         [[-4.9561e-02, -5.0232e-02, -2.4414e-04,  ..., -1.7120e-02,\n",
      "           -3.0640e-02, -5.8289e-02],\n",
      "          [-8.9233e-02, -9.5093e-02,  7.3608e-02,  ..., -6.0547e-02,\n",
      "           -8.6792e-02, -1.1157e-01],\n",
      "          [-9.0332e-02, -5.7159e-02, -8.5144e-03,  ..., -1.0809e-01,\n",
      "           -1.0724e-01, -8.2520e-02],\n",
      "          ...,\n",
      "          [-9.9854e-02, -5.9021e-02, -6.4819e-02,  ..., -1.4001e-01,\n",
      "           -1.7529e-01, -1.2964e-01],\n",
      "          [-8.7769e-02, -8.6670e-02, -6.8970e-02,  ..., -1.4160e-01,\n",
      "           -1.6431e-01, -1.1169e-01],\n",
      "          [-6.0150e-02, -1.6907e-02,  2.7222e-02,  ..., -1.7578e-01,\n",
      "           -1.9568e-01, -1.5552e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.0566e-03, -4.5349e-02, -6.5491e-02,  ..., -3.3234e-02,\n",
      "           -4.1626e-02, -5.8716e-02],\n",
      "          [ 7.3853e-03, -1.7426e-02, -7.5989e-02,  ..., -3.7201e-02,\n",
      "           -4.7241e-02, -7.4768e-02],\n",
      "          [-2.6398e-02,  1.9424e-02,  1.4969e-02,  ..., -1.6205e-02,\n",
      "           -2.1149e-02, -5.0781e-02],\n",
      "          ...,\n",
      "          [ 4.3762e-02,  1.4795e-01,  8.9294e-02,  ...,  3.0319e-02,\n",
      "            6.2256e-03,  2.1133e-02],\n",
      "          [ 1.4816e-02,  7.8186e-02,  3.8940e-02,  ...,  6.4636e-02,\n",
      "            3.0014e-02, -1.9226e-03],\n",
      "          [ 1.9791e-02,  5.0964e-03, -1.7487e-02,  ...,  4.9377e-02,\n",
      "            3.7292e-02,  2.2537e-02]],\n",
      "\n",
      "         [[-2.9327e-02,  1.2238e-02,  4.1199e-04,  ..., -1.9348e-02,\n",
      "           -4.2542e-02,  2.5345e-02],\n",
      "          [ 1.4420e-02,  4.1077e-02,  8.7402e-02,  ..., -1.0083e-01,\n",
      "           -1.0480e-01, -1.8127e-02],\n",
      "          [ 4.0894e-02,  9.2834e-02,  7.9346e-02,  ..., -4.6692e-02,\n",
      "           -1.0968e-01, -3.1708e-02],\n",
      "          ...,\n",
      "          [-1.8692e-02, -3.7964e-02, -5.3467e-02,  ..., -7.6294e-02,\n",
      "           -1.0059e-01, -2.0096e-02],\n",
      "          [-3.7659e-02, -2.1347e-02, -5.2124e-02,  ..., -1.0095e-01,\n",
      "           -9.8816e-02, -1.9714e-02],\n",
      "          [-6.3477e-02, -2.9602e-03, -8.6441e-03,  ..., -5.2124e-02,\n",
      "           -7.0374e-02, -2.1667e-02]],\n",
      "\n",
      "         [[-1.6159e-02, -1.6327e-02, -3.8574e-02,  ..., -3.7506e-02,\n",
      "            1.6891e-02,  3.2898e-02],\n",
      "          [-4.5319e-03,  9.9335e-03,  2.5215e-03,  ..., -6.8359e-02,\n",
      "           -6.9962e-03,  6.6162e-02],\n",
      "          [ 5.1880e-02,  5.8899e-02,  4.9957e-02,  ...,  4.4708e-02,\n",
      "            8.1421e-02,  8.7524e-02],\n",
      "          ...,\n",
      "          [ 3.9459e-02,  4.5654e-02,  1.6846e-02,  ...,  4.4586e-02,\n",
      "            4.6326e-02,  2.1072e-02],\n",
      "          [ 4.5990e-02,  2.2263e-02, -8.1635e-03,  ..., -5.0850e-03,\n",
      "            1.6541e-02,  1.8158e-02],\n",
      "          [ 3.4142e-03, -3.6259e-03, -1.0086e-02,  ...,  2.0416e-02,\n",
      "            2.7618e-02,  2.1530e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6327e-02,  7.0251e-02,  7.5806e-02,  ...,  9.1675e-02,\n",
      "            1.1133e-01,  1.0315e-01],\n",
      "          [ 5.2612e-02,  9.6558e-02,  2.2583e-02,  ...,  3.8574e-02,\n",
      "           -3.9978e-03, -2.6947e-02],\n",
      "          [ 2.6398e-02,  6.2622e-02,  1.1047e-01,  ...,  3.1708e-02,\n",
      "            6.0577e-02, -3.7445e-02],\n",
      "          ...,\n",
      "          [ 5.3772e-02,  6.4514e-02,  5.2094e-02,  ...,  3.9734e-02,\n",
      "            1.6418e-02,  4.5837e-02],\n",
      "          [ 5.4596e-02,  4.4678e-02,  2.3376e-02,  ...,  1.0278e-01,\n",
      "            4.2389e-02,  4.4281e-02],\n",
      "          [ 6.5979e-02,  3.8513e-02,  6.1005e-02,  ...,  3.9062e-02,\n",
      "            2.4689e-02,  3.2043e-02]],\n",
      "\n",
      "         [[ 1.4542e-02,  1.9363e-02, -1.6693e-02,  ..., -7.4341e-02,\n",
      "           -5.3375e-02, -2.9205e-02],\n",
      "          [-6.9275e-03, -5.7983e-04,  3.9001e-02,  ..., -8.4595e-02,\n",
      "           -1.3611e-01, -6.5552e-02],\n",
      "          [-1.3580e-02, -2.0752e-02,  7.0801e-03,  ..., -3.4332e-02,\n",
      "           -1.0388e-01, -6.0333e-02],\n",
      "          ...,\n",
      "          [ 7.4707e-02,  1.0266e-01,  5.8533e-02,  ...,  8.4412e-02,\n",
      "            7.3853e-02,  6.2408e-03],\n",
      "          [ 4.5715e-02,  6.1920e-02,  3.2288e-02,  ...,  7.7393e-02,\n",
      "            6.7871e-02,  1.7639e-02],\n",
      "          [-4.1809e-03,  2.6672e-02,  1.1581e-02,  ...,  1.0413e-01,\n",
      "            6.4270e-02,  2.5604e-02]],\n",
      "\n",
      "         [[-5.3406e-02, -1.3428e-02,  1.3123e-02,  ..., -6.9336e-02,\n",
      "           -9.5093e-02, -1.3293e-01],\n",
      "          [-5.6030e-02,  3.1189e-02,  8.5327e-02,  ..., -2.5192e-02,\n",
      "           -8.2520e-02, -3.3569e-03],\n",
      "          [-5.9509e-02,  4.0710e-02,  4.4250e-02,  ...,  7.4585e-02,\n",
      "           -1.1902e-02, -2.2278e-03],\n",
      "          ...,\n",
      "          [-6.3354e-02, -7.5378e-02, -7.2510e-02,  ..., -2.8183e-02,\n",
      "           -1.1932e-01, -7.9346e-02],\n",
      "          [-6.1768e-02, -3.5095e-03, -3.1097e-02,  ..., -3.0228e-02,\n",
      "           -8.0078e-02, -7.8491e-02],\n",
      "          [-2.7924e-02,  3.5156e-02, -1.4526e-02,  ..., -6.9275e-02,\n",
      "           -6.7749e-02, -4.6173e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5060e-02, -1.5259e-02, -1.7395e-02,  ..., -8.0627e-02,\n",
      "           -7.0618e-02,  4.2725e-04],\n",
      "          [ 2.7481e-02, -4.6173e-02, -6.6956e-02,  ..., -1.0052e-01,\n",
      "            1.3992e-02, -7.1716e-04],\n",
      "          [ 4.3579e-02, -6.4636e-02, -1.2036e-01,  ..., -1.0120e-01,\n",
      "           -2.0172e-02, -4.1138e-02],\n",
      "          ...,\n",
      "          [-9.8694e-02, -8.7402e-02, -1.7227e-02,  ...,  1.2436e-02,\n",
      "            6.6528e-02, -3.6255e-02],\n",
      "          [-8.8867e-02, -5.4016e-02, -1.4877e-02,  ..., -1.0223e-02,\n",
      "            9.4147e-03, -4.7363e-02],\n",
      "          [-6.7749e-02, -7.2021e-02, -3.4790e-02,  ..., -3.7354e-02,\n",
      "            1.0559e-02,  2.0554e-02]],\n",
      "\n",
      "         [[ 2.1362e-04,  6.9885e-02,  6.3721e-02,  ...,  3.7476e-02,\n",
      "           -1.0635e-02, -8.2947e-02],\n",
      "          [ 1.6586e-02,  4.0344e-02,  5.9631e-02,  ...,  3.2288e-02,\n",
      "           -6.6162e-02, -2.5589e-02],\n",
      "          [ 2.2934e-02,  6.5308e-02,  1.0510e-01,  ...,  6.4209e-02,\n",
      "            3.0777e-02, -4.9377e-02],\n",
      "          ...,\n",
      "          [-4.1077e-02, -4.5959e-02, -8.2886e-02,  ..., -2.4216e-02,\n",
      "           -3.5950e-02, -1.8005e-02],\n",
      "          [-1.8311e-02, -5.0171e-02, -7.7637e-02,  ..., -4.0588e-03,\n",
      "           -4.3549e-02, -3.0121e-02],\n",
      "          [ 2.1362e-03,  3.2227e-02,  1.0544e-02,  ..., -2.3285e-02,\n",
      "           -2.1378e-02, -3.2532e-02]],\n",
      "\n",
      "         [[ 3.2043e-02,  8.1177e-03, -8.1482e-02,  ..., -5.8685e-02,\n",
      "            1.7944e-02,  4.0344e-02],\n",
      "          [-6.8588e-03, -2.4170e-02, -1.1951e-01,  ..., -7.2021e-02,\n",
      "           -7.6721e-02, -4.4525e-02],\n",
      "          [-8.8684e-02, -6.4636e-02, -1.1877e-01,  ..., -2.1027e-02,\n",
      "            2.6794e-02, -1.7899e-02],\n",
      "          ...,\n",
      "          [ 1.0872e-03, -3.8086e-02,  1.5381e-02,  ..., -8.0750e-02,\n",
      "           -1.3892e-01, -2.2888e-02],\n",
      "          [ 3.0174e-03, -7.6172e-02, -6.5918e-02,  ..., -7.8735e-02,\n",
      "           -1.0315e-01, -3.5309e-02],\n",
      "          [ 1.4381e-03, -2.2537e-02, -1.6434e-02,  ..., -4.2450e-02,\n",
      "           -3.4515e-02, -7.5302e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.2512e-02,  1.1523e-01,  1.0327e-01,  ...,  1.1102e-01,\n",
      "            1.2463e-01,  1.4709e-01],\n",
      "          [ 2.9526e-02,  1.8188e-02,  1.9348e-02,  ...,  2.7649e-02,\n",
      "            1.8921e-03,  6.9397e-02],\n",
      "          [ 2.3743e-02,  5.3406e-02, -3.0914e-02,  ...,  5.6671e-02,\n",
      "            2.5726e-02,  2.7344e-02],\n",
      "          ...,\n",
      "          [ 1.8829e-02,  1.4221e-02,  1.1096e-01,  ...,  5.3528e-02,\n",
      "            2.2888e-03,  1.0132e-02],\n",
      "          [ 4.5959e-02,  3.6194e-02,  8.9355e-02,  ...,  9.0942e-03,\n",
      "            2.4994e-02,  4.8462e-02],\n",
      "          [ 7.7881e-02,  2.9907e-02,  5.5542e-02,  ...,  2.2034e-02,\n",
      "           -3.0518e-05, -5.8289e-03]],\n",
      "\n",
      "         [[-2.2186e-02, -7.8735e-02,  6.2103e-03,  ...,  1.5015e-02,\n",
      "            5.9326e-02,  4.6326e-02],\n",
      "          [-1.5320e-02, -2.5848e-02,  3.4088e-02,  ...,  8.0322e-02,\n",
      "            3.9612e-02,  4.6021e-02],\n",
      "          [-2.2797e-02, -3.3356e-02, -2.5543e-02,  ...,  9.9365e-02,\n",
      "            8.2703e-02,  3.7750e-02],\n",
      "          ...,\n",
      "          [-5.8380e-02,  1.9531e-02, -4.5929e-02,  ..., -2.7252e-02,\n",
      "            7.6904e-03,  4.5990e-02],\n",
      "          [-8.3862e-02, -5.0812e-02, -6.9336e-02,  ...,  7.4310e-03,\n",
      "            1.7838e-02,  8.5602e-03],\n",
      "          [-4.9591e-02, -6.1615e-02, -5.3375e-02,  ..., -2.7557e-02,\n",
      "           -9.4910e-03, -6.2561e-03]],\n",
      "\n",
      "         [[-1.3867e-01, -9.3262e-02, -9.6008e-02,  ..., -5.4138e-02,\n",
      "           -4.9835e-02, -1.0040e-01],\n",
      "          [-6.7688e-02, -1.5228e-02, -4.1809e-02,  ...,  5.0476e-02,\n",
      "            4.5654e-02, -3.1586e-02],\n",
      "          [ 9.0637e-03,  1.1499e-01, -1.2207e-04,  ...,  2.4475e-02,\n",
      "           -7.0496e-03, -3.8452e-02],\n",
      "          ...,\n",
      "          [-1.8188e-01, -1.3208e-01, -1.4624e-01,  ..., -1.9153e-01,\n",
      "           -1.5405e-01, -7.2021e-02],\n",
      "          [-1.7078e-01, -1.0132e-01, -1.2482e-01,  ..., -1.8860e-01,\n",
      "           -1.3672e-01, -5.7343e-02],\n",
      "          [-1.1523e-01, -1.0364e-01, -7.5562e-02,  ..., -2.2385e-02,\n",
      "           -5.7678e-02,  9.0637e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.6448e-02, -7.3547e-02, -2.8763e-02,  ..., -5.0873e-02,\n",
      "           -4.2023e-02, -2.8625e-02],\n",
      "          [-4.9927e-02,  1.5869e-03,  3.9246e-02,  ..., -5.2246e-02,\n",
      "           -1.0605e-02, -4.7424e-02],\n",
      "          [-5.9998e-02,  4.8828e-04,  1.2619e-02,  ..., -9.8938e-02,\n",
      "           -5.1270e-02, -5.3345e-02],\n",
      "          ...,\n",
      "          [ 4.3335e-02,  7.0190e-02,  5.1208e-02,  ...,  1.0498e-01,\n",
      "            2.7786e-02, -6.2073e-02],\n",
      "          [ 3.3325e-02,  1.0010e-01,  7.6172e-02,  ...,  1.0730e-01,\n",
      "            2.3727e-02, -5.3040e-02],\n",
      "          [ 2.8854e-02,  8.2031e-02,  8.4961e-02,  ...,  6.8726e-02,\n",
      "            9.7107e-02,  2.0340e-02]],\n",
      "\n",
      "         [[-5.9692e-02, -1.7303e-02, -5.4382e-02,  ..., -5.6610e-03,\n",
      "           -1.0078e-02, -4.7913e-03],\n",
      "          [-1.8555e-02,  7.8491e-02, -4.7974e-02,  ...,  5.9631e-02,\n",
      "            3.5034e-02,  5.7556e-02],\n",
      "          [ 3.1799e-02,  7.1228e-02,  1.2695e-02,  ...,  6.7383e-02,\n",
      "            1.8021e-02, -8.1482e-03],\n",
      "          ...,\n",
      "          [-7.3975e-02, -1.6138e-01, -3.4851e-02,  ..., -8.7219e-02,\n",
      "           -9.6375e-02, -1.4297e-02],\n",
      "          [-4.3793e-02, -1.4087e-01, -1.5327e-02,  ..., -1.0638e-01,\n",
      "           -6.2683e-02, -7.7591e-03],\n",
      "          [ 2.3804e-03, -3.6652e-02, -1.6830e-02,  ..., -1.5251e-02,\n",
      "            6.8665e-04,  3.6438e-02]],\n",
      "\n",
      "         [[ 1.8112e-02,  1.0010e-01,  6.0547e-02,  ..., -7.2266e-02,\n",
      "           -1.8585e-02,  1.1200e-02],\n",
      "          [ 3.0426e-02,  7.4097e-02,  5.7495e-02,  ..., -4.9316e-02,\n",
      "           -8.4167e-02, -7.5073e-02],\n",
      "          [ 3.9124e-02, -1.2207e-02,  2.5269e-02,  ..., -3.6316e-02,\n",
      "           -4.3854e-02, -8.2947e-02],\n",
      "          ...,\n",
      "          [-2.2842e-02, -2.3300e-02,  9.3506e-02,  ...,  2.3694e-01,\n",
      "            1.4453e-01,  4.8248e-02],\n",
      "          [-1.5518e-02,  2.8931e-02,  9.7412e-02,  ...,  1.1603e-01,\n",
      "            1.5906e-01,  3.7445e-02],\n",
      "          [ 1.1154e-02,  5.7861e-02,  8.2031e-02,  ...,  4.9530e-02,\n",
      "            4.8096e-02,  3.1036e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8311e-03,  9.6069e-02,  4.6631e-02,  ..., -3.1738e-03,\n",
      "           -1.5900e-02,  3.4973e-02],\n",
      "          [ 2.8000e-02,  9.1125e-02,  1.1652e-01,  ...,  5.3986e-02,\n",
      "           -3.0212e-03, -1.7090e-03],\n",
      "          [ 4.5227e-02,  8.9783e-02,  1.7090e-01,  ...,  1.3477e-01,\n",
      "            7.1777e-02, -2.3346e-02],\n",
      "          ...,\n",
      "          [ 9.7290e-02,  1.0162e-01,  4.9561e-02,  ...,  1.2769e-01,\n",
      "            1.2000e-01,  4.0100e-02],\n",
      "          [ 9.8022e-02,  3.9307e-02,  9.0637e-03,  ...,  6.8115e-02,\n",
      "            9.0698e-02,  2.9358e-02],\n",
      "          [ 1.2439e-01,  7.5073e-02,  8.3740e-02,  ...,  6.2469e-02,\n",
      "            8.2764e-02,  6.7566e-02]],\n",
      "\n",
      "         [[ 4.7241e-02,  5.2185e-02,  6.8512e-03,  ...,  4.1931e-02,\n",
      "            1.7502e-02,  3.7964e-02],\n",
      "          [ 3.8818e-02,  9.0454e-02,  9.5398e-02,  ...,  2.5146e-02,\n",
      "           -3.8483e-02,  6.0120e-03],\n",
      "          [ 4.4617e-02,  8.6487e-02,  8.8989e-02,  ...,  6.0059e-02,\n",
      "           -9.9792e-03,  3.3356e-02],\n",
      "          ...,\n",
      "          [ 8.7036e-02,  8.4106e-02,  6.9458e-02,  ...,  8.7463e-02,\n",
      "            7.5195e-02,  9.3994e-02],\n",
      "          [ 9.9976e-02,  1.0303e-01,  5.1880e-02,  ...,  7.1472e-02,\n",
      "            6.2988e-02,  1.1157e-01],\n",
      "          [ 4.9469e-02,  8.8379e-02,  3.6438e-02,  ...,  3.1082e-02,\n",
      "            1.4435e-02,  4.7852e-02]],\n",
      "\n",
      "         [[-2.2293e-02,  9.2468e-03, -1.1047e-02,  ..., -4.8157e-02,\n",
      "           -4.0100e-02, -4.7455e-02],\n",
      "          [-4.9683e-02, -2.7176e-02, -4.3732e-02,  ..., -3.3203e-02,\n",
      "           -3.4393e-02, -3.1372e-02],\n",
      "          [-9.9426e-02, -2.6855e-02, -1.1169e-02,  ..., -1.8311e-04,\n",
      "           -5.4962e-02, -6.2561e-03],\n",
      "          ...,\n",
      "          [-4.6814e-02, -5.1697e-02, -1.3916e-01,  ..., -1.6266e-02,\n",
      "            4.6387e-02, -3.4729e-02],\n",
      "          [-2.0721e-02, -5.9998e-02, -1.5723e-01,  ...,  6.2561e-03,\n",
      "            7.1411e-02, -4.4952e-02],\n",
      "          [-4.3640e-02, -9.8755e-02, -1.0498e-01,  ..., -3.6896e-02,\n",
      "           -4.8950e-02, -1.0333e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.8706e-02, -5.5969e-02, -6.5186e-02,  ...,  7.8125e-03,\n",
      "           -2.2247e-02, -4.2725e-03],\n",
      "          [-4.8706e-02, -9.2529e-02, -1.2866e-01,  ...,  5.7922e-02,\n",
      "            1.7197e-02, -7.2876e-02],\n",
      "          [-2.5116e-02, -8.4534e-02, -1.0602e-01,  ..., -2.1866e-02,\n",
      "            2.2736e-03, -1.0803e-01],\n",
      "          ...,\n",
      "          [ 1.4206e-02,  1.5106e-03,  7.1869e-03,  ..., -1.3049e-01,\n",
      "           -1.0760e-01, -1.0901e-01],\n",
      "          [ 3.5645e-02,  5.5237e-02,  7.4463e-02,  ..., -3.5645e-02,\n",
      "           -7.0679e-02, -7.2144e-02],\n",
      "          [-4.6234e-03,  3.4912e-02,  6.4636e-02,  ..., -2.2125e-03,\n",
      "           -2.1210e-02, -7.2510e-02]],\n",
      "\n",
      "         [[-2.0660e-02,  2.2873e-02,  1.2970e-03,  ...,  2.0981e-02,\n",
      "            2.4460e-02,  1.8173e-02],\n",
      "          [ 8.4686e-03,  7.0496e-02, -1.7838e-02,  ..., -2.3346e-03,\n",
      "           -1.0422e-02, -1.6388e-02],\n",
      "          [-2.5772e-02,  3.1982e-02, -1.2405e-02,  ..., -4.3579e-02,\n",
      "           -4.4891e-02,  2.2125e-03],\n",
      "          ...,\n",
      "          [-6.2256e-03,  6.5308e-03,  4.0955e-02,  ...,  9.5520e-03,\n",
      "           -1.8143e-02,  1.0483e-02],\n",
      "          [-5.3833e-02, -1.9211e-02,  8.4381e-03,  ...,  5.6213e-02,\n",
      "            3.8940e-02,  6.9153e-02],\n",
      "          [-9.7275e-03, -6.0425e-02,  1.0681e-04,  ..., -7.1945e-03,\n",
      "           -2.2034e-02,  8.2703e-03]],\n",
      "\n",
      "         [[ 3.6041e-02, -8.2550e-03, -1.8097e-02,  ...,  7.0496e-02,\n",
      "            9.1614e-02,  4.0375e-02],\n",
      "          [ 4.9957e-02, -2.5314e-02, -3.3295e-02,  ..., -8.9539e-02,\n",
      "           -7.0007e-02, -1.1536e-02],\n",
      "          [ 2.3010e-02, -3.3081e-02, -3.1311e-02,  ..., -9.7046e-02,\n",
      "           -1.1078e-01,  1.1246e-02],\n",
      "          ...,\n",
      "          [ 1.8799e-02,  8.0627e-02,  9.7168e-02,  ...,  4.3274e-02,\n",
      "            8.0872e-02,  1.7609e-02],\n",
      "          [-2.6031e-02,  1.0330e-02,  1.8402e-02,  ...,  7.1045e-02,\n",
      "            1.1755e-01,  4.9316e-02],\n",
      "          [-3.0701e-02, -3.2043e-02,  2.0561e-03,  ..., -1.3474e-02,\n",
      "            3.2013e-02,  2.3880e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.2471e-02,  5.1483e-02,  8.3496e-02,  ...,  9.2102e-02,\n",
      "            1.6968e-02,  5.7617e-02],\n",
      "          [ 4.5868e-02,  4.9133e-02,  3.8483e-02,  ...,  1.0010e-01,\n",
      "            5.6244e-02,  5.2979e-02],\n",
      "          [ 1.9623e-02,  3.6591e-02,  7.4280e-02,  ...,  9.6069e-02,\n",
      "            9.1553e-02,  5.4047e-02],\n",
      "          ...,\n",
      "          [ 2.5421e-02,  3.3386e-02,  5.0446e-02,  ...,  8.0811e-02,\n",
      "            1.0376e-01,  8.5144e-02],\n",
      "          [ 3.2104e-02,  1.1963e-02,  5.7068e-03,  ...,  9.1309e-02,\n",
      "            8.4534e-02,  5.2856e-02],\n",
      "          [ 5.7709e-02,  3.4546e-02,  4.1382e-02,  ...,  3.8513e-02,\n",
      "            7.0007e-02,  1.5106e-02]],\n",
      "\n",
      "         [[ 2.7588e-02,  3.7048e-02, -1.6541e-02,  ..., -7.4158e-03,\n",
      "           -5.9509e-03,  1.4816e-02],\n",
      "          [ 4.5898e-02,  8.6060e-02,  8.8623e-02,  ...,  4.7485e-02,\n",
      "           -5.8899e-03,  2.3071e-02],\n",
      "          [ 6.9122e-03, -2.5757e-02, -5.0568e-02,  ...,  3.9551e-02,\n",
      "           -1.6388e-02,  2.2705e-02],\n",
      "          ...,\n",
      "          [ 5.5420e-02,  4.0100e-02,  9.8267e-03,  ...,  2.4353e-02,\n",
      "           -9.3079e-03,  4.6997e-03],\n",
      "          [ 1.4069e-02,  2.8915e-02,  2.2827e-02,  ...,  6.1768e-02,\n",
      "            5.9509e-03, -1.4679e-02],\n",
      "          [-5.8289e-03, -1.5137e-02,  1.9226e-02,  ...,  9.2285e-02,\n",
      "            7.1899e-02,  2.8564e-02]],\n",
      "\n",
      "         [[-2.0920e-02, -6.0547e-02, -9.7595e-02,  ..., -6.5369e-02,\n",
      "            1.6174e-03, -1.4496e-02],\n",
      "          [-1.0870e-01, -1.8372e-01, -1.5039e-01,  ..., -1.6693e-02,\n",
      "            2.5269e-02, -3.9551e-02],\n",
      "          [-1.2891e-01, -1.1963e-01, -9.1797e-02,  ..., -3.0823e-02,\n",
      "            2.0447e-03, -3.5339e-02],\n",
      "          ...,\n",
      "          [-5.9113e-02, -2.2110e-02, -1.2299e-01,  ..., -1.2512e-01,\n",
      "           -1.3123e-01, -6.9946e-02],\n",
      "          [-6.6711e-02, -3.2379e-02, -1.6760e-01,  ..., -1.2988e-01,\n",
      "           -1.1487e-01, -7.0496e-02],\n",
      "          [-4.3732e-02, -5.2734e-02, -1.3599e-01,  ..., -2.5528e-02,\n",
      "           -2.7496e-02, -5.2704e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9099e-02,  4.1260e-02,  1.3168e-02,  ..., -2.0386e-02,\n",
      "           -2.3010e-02, -6.1035e-05],\n",
      "          [ 2.5955e-02,  1.3199e-02, -3.6957e-02,  ..., -4.5807e-02,\n",
      "           -2.4902e-02, -6.4514e-02],\n",
      "          [ 3.6133e-02, -1.5732e-02, -7.4951e-02,  ..., -4.1443e-02,\n",
      "           -4.3823e-02, -8.5327e-02],\n",
      "          ...,\n",
      "          [-1.1292e-03, -4.0527e-02, -1.1963e-02,  ...,  5.2490e-02,\n",
      "            5.8655e-02,  2.7786e-02],\n",
      "          [ 3.8208e-02, -3.5553e-03,  7.8583e-03,  ...,  2.5345e-02,\n",
      "            3.6377e-02, -2.2736e-02],\n",
      "          [ 4.9133e-02,  4.6753e-02,  6.4697e-02,  ...,  4.9591e-03,\n",
      "           -4.7150e-03, -3.2654e-02]],\n",
      "\n",
      "         [[-2.3758e-02, -4.1870e-02, -3.3813e-02,  ...,  4.6143e-02,\n",
      "            6.6528e-02,  1.5976e-02],\n",
      "          [ 1.9028e-02, -6.4941e-02, -3.9917e-02,  ...,  7.5989e-02,\n",
      "            9.8389e-02, -7.8583e-03],\n",
      "          [-1.6174e-03, -4.7241e-02,  3.7964e-02,  ...,  3.5828e-02,\n",
      "            1.2878e-01, -1.2054e-03],\n",
      "          ...,\n",
      "          [-6.6711e-02, -8.8501e-04,  2.6749e-02,  ..., -7.6416e-02,\n",
      "           -9.5276e-02, -9.0271e-02],\n",
      "          [-5.5298e-02, -3.4851e-02, -5.6076e-03,  ..., -6.6895e-02,\n",
      "           -5.6213e-02, -6.9092e-02],\n",
      "          [-1.2207e-02, -1.8234e-02, -1.2207e-03,  ..., -3.4210e-02,\n",
      "            3.0289e-02,  6.6528e-03]],\n",
      "\n",
      "         [[-2.0828e-02, -3.4241e-02, -3.8910e-02,  ..., -4.6082e-02,\n",
      "            9.1171e-04,  5.0163e-03],\n",
      "          [-1.3046e-03, -3.3020e-02, -8.2520e-02,  ..., -8.7158e-02,\n",
      "           -5.2399e-02, -2.8870e-02],\n",
      "          [ 2.1545e-02,  1.5442e-02,  2.7863e-02,  ..., -1.2073e-01,\n",
      "           -1.3318e-01, -8.8623e-02],\n",
      "          ...,\n",
      "          [ 4.6906e-02,  5.1300e-02,  2.4368e-02,  ..., -5.9082e-02,\n",
      "            2.1973e-02,  1.4908e-02],\n",
      "          [ 2.5040e-02,  4.6959e-03, -1.1185e-02,  ..., -4.8767e-02,\n",
      "           -3.3951e-03,  1.5823e-02],\n",
      "          [ 1.4603e-02,  1.0147e-02,  2.6245e-02,  ..., -2.4643e-02,\n",
      "            1.3657e-02,  1.0666e-02]]]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([64, 100, 56, 56])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8d60780800e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[1;34m\"nope\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mkl_anneal_portion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: exceptions must derive from BaseException"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "global_step"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6480"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}